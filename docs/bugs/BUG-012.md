# BUG-012: Oversized Chunks & Embedding Dimension Violations (Reduce Chunk Size; Enforce ≤1000-Dim Vectors)

**Status:** Open  
**Component(s):** Ingestion Pipeline (6‑Pass), Vector Upsert (AstraDB), Embedding Service  
**Priority:** P1 (Data Integrity + Cost)  
**Owner:** Ingestion Lead / Data Platform

---

## Summary
In the current 6‑Pass ingestion, some text chunks are **too large** and their embeddings **exceed the JSON API’s 1000‑dimension cap** in AstraDB, causing upsert failures and elevated token costs. We must (a) **enforce compact chunks** (~300–500 characters) and (b) **guarantee vector compatibility** (≤1000 dims when using JSON API arrays, or migrate to a proper vector column).

---

## Context (6‑Pass Pipeline)
- **Pass A** — Initial TOC parse to prime dictionary.  
- **Pass B** — Split files >25 MB by logical sections (chapters).  
- **Pass C** — Parse/segment with Unstructured (layout‑aware elements, max_characters=2000).  
- **Pass D** — Vectorize + enrich/normalize (Haystack) + dictionary delta.  
- **Pass E** — Graph compilation (LlamaIndex).  
- **Pass F** — Cleanup/compaction + manifests.

Problem spans **C→D**: Unstructured may yield long elements (up to 2000 chars); D vectorizes directly without re-splitting, causing 1536-dim embedding failures with AstraDB JSON arrays (1000-dim limit).

---

## Impact
- **Data loss / light DB**: vector upserts fail when dimension > 1000 (JSON API array field).  
- **High token spend**: long chunks produce costly embeddings and LLM contexts.  
- **Latency & recall variance**: oversized chunks degrade retrieval quality and re‑ranking.

---

## Root Causes
1. **Oversized initial chunks**: Pass C configured with max_characters=2000, much larger than target 500 chars; Pass D does not re-split before vectorization.  
2. **Embedding model/collection mismatch**: vector size (1536) exceeds JSON array limit (1000) when not using a native vector column.  
3. **Missing preflight** to block runs with incompatible vector dimension or chunk size distribution.

---

## Goals / Acceptance Criteria
- **G1 — Chunk Size**: ≥95% of chunks are **≤500 characters**; 100% **≤600 characters**; target average **350–450**.  
- **G2 — Vector Dimension**: 100% of embeddings **≤1000 dims** **when using JSON API arrays** *or* collection uses a **native vector column** sized to the model (e.g., 1536).  
- **G3 — Cost/Perf**: Embedding token usage reduced ≥30% vs. last baseline on same corpus; ingest wall‑time not worse by >10%.  
- **G4 — Safety Gates**: Pipeline **fails fast** before DB writes if (a) chunk length distribution violates thresholds, or (b) embedding dim incompatible with collection.

---

## Proposed Fix (Design)
### 0) Immediate: Reduce Pass C max_characters
Update `src_common/pass_c_extraction.py` line 144 from `max_characters=2000` to `max_characters=600` as interim fix.

### 1) Length‑Normalizer in **Pass D**
Add a deterministic splitter layer **after Unstructured** (Pass C) and **before vectorization** (Pass D):
- **Strategy:** word/sentence‑aware split with overlap.  
- **Defaults (configurable):**  
  - `CHUNK_MAX_CHARS=500`  
  - `CHUNK_HARD_CAP=600` (final guard)  
  - `CHUNK_MIN_CHARS=120` (merge upward if sub‑min)  
  - `CHUNK_OVERLAP=60`  
  - `SPLIT_BY="word"` (preferred) or `"sentence"` fallback  
- **Behavior:**  
  1) Normalize whitespace → collapse runs.  
  2) Split by `SPLIT_BY`; assemble windows up to `CHUNK_MAX_CHARS`.  
  3) If any window > `CHUNK_HARD_CAP`, re‑split forcibly on token boundaries.  
  4) Preserve metadata (`page`, `section`, `type`, `source_id`) and compute `char_len`, `token_est`.

### 2) Embedding Preflight in **Pass D**
- Compute `MODEL_DIM` from embedding provider (currently 1536 for OpenAI text-embedding-ada-002).  
- Read collection config:  
  - **Mode A (JSON arrays):** enforce `MODEL_DIM ≤ 1000`.  
  - **Mode B (Vector column):** require collection vector column size == `MODEL_DIM`.  
- If incompatible → **abort run** with actionable error before any DB writes.

### 3) Optional Dimensionality Reduction (only if staying on JSON arrays)
- If we must keep a >1000‑dim model while using JSON arrays, apply **PCA/SVD** to **project to 768/1000 dims** prior to upsert.  
- Gate behind `EMBED_DIM_REDUCTION=off|pca-768|pca-1000`.

### 4) Config & Flags
```
# pass_d_normalizer.env
CHUNK_MAX_CHARS=500
CHUNK_HARD_CAP=600
CHUNK_MIN_CHARS=120
CHUNK_OVERLAP=60
SPLIT_BY=word

# pass_d_vectorization.env  
VECTOR_BACKEND=json|astra_vector
MODEL_DIM=1536  # Current: OpenAI text-embedding-ada-002
EMBED_DIM_REDUCTION=off|pca-768|pca-1000
ABORT_ON_INCOMPATIBLE_VECTOR=true
```

### 5) Schema / Collection Choice
- **Preferred:** create collections with **native vector column** sized to `MODEL_DIM` (e.g., 1536) to avoid JSON limit.  
- **Interim:** if staying with JSON arrays, **pin MODEL_DIM ≤ 1000**.

---

## Implementation Notes (Pseudocode)
```python
# Pass D — length normalizer (before vectorization)
def normalize_chunks(raw_chunks, cfg):
    """Normalize chunks from Pass C before vectorization"""
    items = []
    for chunk in raw_chunks:  # from Pass C (Unstructured)
        text = normalize_ws(chunk["text"])
        if len(text) <= cfg.max_chars:
            items.append(chunk)  # Already compliant
        else:
            # Split oversized chunks
            for win in window_split(text, max_chars=cfg.max_chars, overlap=cfg.overlap):
                if len(win) > cfg.hard_cap:
                    for sub in force_split(win, hard_cap=cfg.hard_cap):
                        items.append(make_chunk_from_parent(chunk, sub))
                else:
                    items.append(make_chunk_from_parent(chunk, win))
    return repair_small(items, min_chars=cfg.min_chars)  # merge tiny tails

# Pass D — vectorization preflight
def preflight_embeddings(model_dim, backend, collection_dim=None):
    if backend == "json" and model_dim > 1000:
        raise SystemExit("Vector dim > 1000 not supported by JSON API arrays")
    if backend == "astra_vector" and collection_dim and collection_dim != model_dim:
        raise SystemExit("Vector column size mismatch")
```

---

## Testing Plan
### Unit
- **Splitter**: produces windows ≤ `CHUNK_MAX_CHARS`; overlap ≈ config; merges sub‑min windows.  
- **Preflight**: rejects `model_dim=1536` with `backend=json`; accepts with `astra_vector` if `collection_dim=1536`.  
- **Token Estimator**: rough tokens/char sanity (< 5% error over sample).

### Functional (Integration)
- **Fixture Book** run through C→D→E:  
  - 100% chunks ≤ 600 chars; ≥95% ≤ 500 chars.  
  - 0 vector upsert errors.  
  - Compare baseline vs. new: ≥30% drop in embedding tokens.

### Regression
- Previous “golden” queries produce equivalent or improved retrieval (top‑k stability within tolerance).  
- No change to schema keys consumed by Phase 2/3.

### Security
- Splitting preserves redaction and does not leak PII across windows.  
- Preflight errors redact secrets from logs.

---

## Rollout
1. **Immediate**: Reduce Pass C max_characters from 2000 to 600 as hotfix.
2. Implement Pass D normalizer + vectorization preflight behind feature flags.  
3. Enable in **DEV**; run against representative corpus; capture metrics.  
4. If VECTOR_BACKEND=`json` and `MODEL_DIM>1000`, choose **either**:  
   - Switch to **astra_vector** with proper vector column, **or**  
   - Turn on `EMBED_DIM_REDUCTION` to `pca-768` (interim).  
5. Promote to **TEST** after green eval; then **PROD**.  
6. **Backfill:** re‑ingest affected documents to regenerate compliant chunks/vectors.

---

## Monitoring & Metrics
- **Chunk length histogram** (p50/p95/max) and **token estimate** per job.  
- **Embedding failures** by reason; **vector dim compatibility** events.  
- **Cost KPIs**: tokens/doc (embed), tokens/query (RAG), total spend per 1k pages.  
- Dashboard tiles in Admin UI (Phase 4) for length compliance & vector health.

---

## Risks & Mitigations
- **Recall drop** from smaller chunks → mitigate with modest overlap (60) + re‑rank (MMR).  
- **Throughput hit** from extra splitting → pipeline parallelism adjusted; target <10% wall‑time regression.  
- **Migration complexity** if moving to vector columns → stage in DEV/TEST; dual‑write optional during cutover.

---

## Definition of Done
- New DEV run shows:  
  - ≥95% chunks ≤500 chars; 100% ≤600 chars.  
  - 0 vector upsert errors; embeddings compatible with chosen backend.  
  - ≥30% reduction in embedding tokens vs. baseline.  
- CI has unit/functional/regression tests covering the above.  
- Runbook updated with new flags and failure modes.
