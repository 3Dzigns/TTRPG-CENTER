# BUG-030 — Lane A Ingestion is Stubbed / Not Executing Passes A–G
**Status:** Open  
**Severity:** S1 – System Blocker  
**Priority:** P0 – Fix before next release  
**Reported:** 2025-09-19 00:39:36 (America/Chicago)  
**Reporter:** Jim D.  
**Area:** Ingestion Pipeline → Lane A (Admin-managed uploads)  
**Environments:** dev, test (probable), prod (N/A)  
**Related:** BUG-028 (Ad‑hoc not running), FR-033 (ingestion jobs console), FR-021 (HGRN/HRGN pass), FR-031 (Cassandra in Docker), FR-032 (Lane/consistency), FR-030-favicon (unrelated, reference only)

---

## Summary
The Lane A ingestion pipeline is currently **stubbed** and does not execute the defined Passes **A–G**. Jobs can be triggered (nightly, ad‑hoc, or single‑file) but the system does not progress beyond a placeholder “task started” state or equivalent stub behavior. No artifacts are produced (no chunks in Cassandra, no metadata updates in MongoDB, no graph in Neo4J, etc.), and temp workspaces are not cleaned.

This blocks ingestion of all **Lane A** sources (Source books, Supplemental books; Modules planned but not yet supported) from the **Admin-managed `/uploads` folder** (PDFs; ePub planned).

---

## Expected Behavior
When a job is triggered for **Lane A** (nightly, ad‑hoc, or single-file), the system must execute **Passes A–G** and produce the corresponding artifacts with complete observability. Successful completion means:

1. **Pre-check (Gate 0):**  
   - Compute file **SHA** and **expected chunk count**; compare to locally stored cache.  
   - If both match, **short‑circuit** Lane A (mark “up‑to‑date”) and exit cleanly with audit log + metrics.  
   - Else, continue.

2. **Pass A – TOC → Metadata via OpenAI (GPT‑5):**  
   - Extract TOC (first few pages).  
   - Call OpenAI to extract structured **document metadata** (title, edition, system, publisher, release date, chapters/sections with page ranges, etc.).  
   - Persist metadata (MongoDB) and attach to working doc record.

3. **Pass B – Size-based Split (>25 MB):**  
   - If source > 25 MB, split **by chapter/section** using TOC hints.  
   - Each part **retains original documentId** and has deterministic partIds.  
   - Record parent↔child relations and part ordering.

4. **Pass C – Unstructured.io → Chunking:**  
   - Send (parts or whole) to **Unstructured.io**.  
   - Produce semantically safe chunks (~**600 chars**, **no sentence breaks**).  
   - Upsert chunks to **Cassandra** with rich metadata (docId, partId, page range, section path, headings, sha, source lane, pass version, etc.).

5. **Pass D – Haystack Enrichment:**  
   - Clean/normalize metadata in **MongoDB**.  
   - Enrich each Cassandra chunk (canonical section names, normalized system keys, edition/versioning, language, permissions/rights, etc.).

6. **Pass E – LlamaIndex Graph:**  
   - Build knowledge **graph** (entities, relations) and persist to **Neo4J**.  
   - Link nodes/edges back to chunk IDs and page refs.

7. **Pass F – Cleanup:**  
   - Remove temp files, orphaned parts, and intermediate artifacts.  
   - Verify no stray files remain in work directories/volumes.

8. **Pass G – HGRN Sanity Check:**  
   - Run **HGRN** sanity checks: chunk coverage vs TOC, dictionary conformance, graph completeness, dangling references, and recommended fixes (persisted to Admin UI queue for accept/reject).

9. **Observability & Exit Criteria:**  
   - Each pass emits logs, counters, and errors surfaced in the Admin console.  
   - Final status = **Success** with artifact counts (chunks written, nodes/edges, metadata updates) or **Failed** with actionable errors.

---

## Actual Behavior
- Job triggers report a generic/stub message (e.g., “Task started…”).  
- No evidence of Passes A–G executing (no progress logs or metrics per pass).  
- No new chunks in Cassandra, no metadata updates in MongoDB, no Neo4J graph updates.  
- Cleanup not performed; temp work dirs may remain unchanged.  
- Admin console lacks real‑time progress and per‑pass visibility for these runs.

---

## Scope / Affected
- **Lane A only** (uploads → PDFs; ePub planned).  
- **Triggers:** nightly scheduled jobs, ad‑hoc runs, single‑file ingestion.  
- **Data stores:** Cassandra, MongoDB, Neo4J.  
- **External services:** OpenAI (GPT‑5), Unstructured.io, LlamaIndex, HGRN.  
- **Runtimes:** Dockerized services for dev/test; orchestrator + workers.

---

## Impact
- New or updated source materials **cannot be ingested**.  
- Downstream features (RAG, graph‑augmented retrieval, dictionary QA) **inoperable**.  
- Admin observability shows misleading success states without artifacts.

---

## Reproduction Steps
1. Place a valid **PDF** in the **Admin uploads** folder. (e.g., `/data/uploads/<system>/<book>.pdf`).  
2. Trigger a **Lane A** job via:  
   - **Nightly schedule** (wait window) **or**  
   - **Ad‑hoc** run from Admin console **or**  
   - **Single‑file** ingestion action.  
3. Observe system logs and Admin console:  
   - You will see a stub/placeholder start message.  
   - No per‑pass logs, no artifact diffs, no counters.  
   - Cassandra/MongoDB/Neo4J show **no new data**.

**Expected:** Completed Passes A–G with artifacts & metrics.  
**Actual:** Run terminates or sits idle with no results.

---

## Diagnostics to Collect
- Orchestrator logs (DEBUG) around job startup and pass dispatch.  
- Worker logs for each pass (A–G).  
- Service logs: Cassandra, MongoDB, Neo4J, Unstructured.io client, OpenAI client, LlamaIndex, HGRN.  
- Admin console network traces (XHR/WebSocket) for progress stream.  
- Job record (jobId, lane, source, env) and any persisted status docs.  
- Contents of temp/work directories before/after run.

---

## Suspected Root Causes
- Pipeline stages compiled but **wired to stub implementations**.  
- Feature flags or env vars leave **passes disabled** in dev.  
- Missing worker containers or misconfigured queues/topics.  
- Credentials/secrets not mounted (OpenAI, Unstructured.io, Neo4J).  
- Incomplete service wiring (e.g., LlamaIndex unable to reach Neo4J).

---

## Acceptance Criteria (Exit Criteria)
- ✅ Lane A runs end‑to‑end with **no stubs** across **dev** and **test**.  
- ✅ Each pass (A–G) **executes** and **emits metrics** visible in the Admin UI.  
- ✅ Short‑circuit path (SHA + chunk‑count match) works with audited “up‑to‑date” status.  
- ✅ Artifacts present & verifiable:  
  - Cassandra: chunks with required metadata fields (docId, partId, section path, pageStart/pageEnd, sha, pass versions, etc.).  
  - MongoDB: normalized document record + enriched metadata.  
  - Neo4J: nodes/edges linked back to chunk IDs and page refs.  
- ✅ Cleanup removes temp files; no orphaned artifacts.  
- ✅ HGRN run produces sanity report with actionable suggestions visible in Admin UI.  
- ✅ Unit + integration tests pass in CI and inside Docker dev.  
- ✅ Documentation updated (Admin + Dev guides).

---

## Test Plan

### Unit Tests
- **Pass A (TOC/Metadata):**  
  - Given a sample TOC, OpenAI extraction returns structured metadata; validate required fields and schema.  
  - Error paths: missing TOC, malformed PDF, OpenAI failure → retries + surfaced error.

- **Pass B (Split):**  
  - Given file sizes, verify split threshold at 25 MB.  
  - Verify chapter‑aligned splits using TOC bounds; parts retain `documentId` and deterministic `partId`s.

- **Pass C (Unstructured + Chunking):**  
  - Chunks are ≤ ~600 chars and never split sentences; include page ranges and section paths.  
  - Upsert merges on `(documentId, partId, pageRange, sha)` without duplicates.

- **Pass D (Haystack Enrichment):**  
  - Metadata normalization functions (system, edition, language, rights) are idempotent.  
  - Enrichment writes back to Cassandra and MongoDB with versioned fields.

- **Pass E (Graph Build):**  
  - Entities/relations extracted; nodes/edges link to chunk IDs and page refs.  
  - Graph schema migration applies cleanly; invalid nodes rejected with reasons.

- **Pass F (Cleanup):**  
  - Temp files/directories removed; idempotent on re‑runs.

- **Pass G (HGRN Sanity):**  
  - Reports coverage vs TOC, dictionary mismatches, dangling references; JSON report schema validated.

- **Gate 0 (SHA/Chunk Count Cache):**  
  - Matching SHA + chunk count → short‑circuit with audited no‑op.  
  - Mismatch → full pipeline executes.

### Integration Tests (Docker, Dev)
1. **Single Small PDF (<25 MB):** Full A–G, no split. Verify all artifacts and UI counters.  
2. **Large PDF (>25 MB):** Split by chapters; verify parent/child links and final merge in metrics.  
3. **Re‑ingest Same File:** Should short‑circuit; verify “up‑to‑date” path.  
4. **Fault Injection (OpenAI down):** Pass A fails gracefully with retries and clear error surfaced.  
5. **Fault Injection (Neo4J offline):** Pass E fails; earlier artifacts remain; job marked failed with remediation steps.  
6. **Ad‑hoc Trigger:** Works end‑to‑end and surfaces live progress.  
7. **Nightly Batch:** Processes multiple files; per‑file isolation & summaries.  
8. **Cleanup Verification:** No stray temp artifacts after success/failure.  
9. **HGRN Report:** Results appear in Admin UI with accept/reject queue.

### Data Validation Queries
- **Cassandra (CQL):** count chunks by `documentId`, verify metadata fields present and non‑null.  
- **MongoDB:** verify document record + normalized fields + TOC map.  
- **Neo4J (Cypher):** ensure nodes/edges exist and reference chunk IDs; sample path queries succeed.

---

## Instrumentation & Observability
- Pass‑scoped **metrics**: start/end timestamps, records processed, output counts, error counts.  
- **Structured logs** with `jobId`, `lane`, `pass`, `documentId`, `partId`.  
- Admin UI: per‑pass progress bar, live logs, counters; final summary report.  
- Job audit trail stored in MongoDB for historical analysis.

---

## Configuration / Env
- **Feature flags:** `LANE_A_ENABLED=true`, per‑pass toggles `PASS_A..PASS_G=true`.  
- **Secrets:** OpenAI, Unstructured.io, Neo4J creds mounted to workers.  
- **Resource limits:** worker concurrency, memory/CPU for Unstructured/LlamaIndex steps.  
- **Paths:** `/data/uploads` (input), `/data/work` (temp), `/data/out` (debug dumps).

---

## Proposed Fix (High Level)
1. Replace stub calls with real implementations for Passes **A–G**.  
2. Wire orchestrator → queue/topics → workers with retry/backoff.  
3. Ensure service clients (OpenAI, Unstructured, Neo4J) are configured and health‑checked at startup.  
4. Implement Gate 0 cache (SHA + chunk count) and audit logging.  
5. Add per‑pass metrics + structured logs + Admin UI progress streaming.  
6. Expand CI to run unit tests; add Docker‑in‑Docker integration job to run the full pipeline on fixtures.  
7. Update docs (Admin/Developer) to reflect final behavior and troubleshooting.

---

## Task Breakdown
- [ ] Orchestrator: enable Lane A route, remove stubs, add Gate 0.  
- [ ] Workers (A–G): implement and register handlers; add health checks.  
- [ ] Clients: OpenAI, Unstructured.io, LlamaIndex, Neo4J connectors & retries.  
- [ ] Datastores: Cassandra upsert schema, MongoDB metadata update, Neo4J schema migration.  
- [ ] Admin UI: progress stream, per‑pass panels, summary + artifact counts.  
- [ ] CI/CD: unit tests, fixture PDFs, integration job in Docker.  
- [ ] Docs: runbooks, config matrix, env/secrets, troubleshooting.

---

## Rollback / Mitigation
- Keep existing artifacts read‑only.  
- If Pass E–G unstable, allow feature‑flagged disable while A–D proceed (with warnings).  
- Provide manual re‑queue capability for failed passes.

---

## Attachments / Notes
- Use sample fixture PDFs: small (10 MB), large (120 MB with 10 chapters).  
- Ensure split preserves `documentId` and deterministic `partId` across runs.  
- Ensure chunking **never breaks sentences**; add tests around edge punctuation and abbreviations.
