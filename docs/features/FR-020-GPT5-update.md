# FR-020 — GPT‑5 Platform Update

**Status:** Proposed  
**Owner:** Engineering (AI Platform)  
**Created:** 2025-09-16  
**Environments:** DEV → TEST → PROD (immutable builds)  
**Related:** FRs touching API client, LangFlow nodes, ingestion assistants, UI chat, Discord bridge

---

## 1) Summary
Update **all non‑embedding LLM calls** from GPT‑4 variants to **GPT‑5 family** with a resilient fallback to GPT‑4. Introduce **tiered model selection** so that “mini” and “nano” are automatically used where latency/cost dominates and quality demands are lower. Preserve a single switch to revert to GPT‑4 in case of incident.

> Embedding models remain unchanged and are **out of scope** for this FR.

---

## 2) Goals / Non‑Goals

### Goals
- Replace default chat/completion models with **GPT‑5** in all non‑embedding paths.
- Route lightweight tasks to **GPT‑5‑mini**; ultra‑light/edge or bursty tasks to **GPT‑5‑nano**.
- Implement a **fallback chain**: `primary → secondary → tertiary` (e.g., `gpt‑5 → gpt‑5‑mini → gpt‑4`).
- Centralize model selection via **config + policy rules** (no hardcoded strings in app code).
- Add **smoke tests** in **DEV** that validate API reachability, auth, and a known prompt/response.
- Update **build artifacts** and **LangFlow** nodes to use the centralized model resolver.
- Add **observability**: model name, latency, tokens, cost estimate (if available), and fallback hops to logs/metrics.

### Non‑Goals
- Changing embedding model(s).
- Re‑writing prompt libraries (beyond metadata adjustments to note model differences).
- Pricing/business policy changes (separate governance).

---

## 3) Scope of Change

**Impacted components**
- Python API client/wrappers: `ttrpg.core.llm_client`, `ttrpg.shared.model_resolver`
- LangFlow custom nodes (non‑embedding): generation, classification, routing, tool‑use
- Discord/FastAPI bridge (reply generation only)
- UI Chat/Agent backends (streaming + tool calling paths)
- Batch/daemon workers (light classification, summarization, index helper tasks)
- CI smoke tests and DEV launch scripts

**Out of scope**
- Embedding pipelines (vectorization), chunkers, and retrieval logic

---

## 4) Design Overview

### 4.1 Central Model Resolver
Introduce a single **ModelResolver** that consumes policy + environment config:

```yaml
# config/models.yaml
defaults:
  primary_model: gpt-5
  classification_model: gpt-5-mini
  routing_model: gpt-5-mini
  ultra_light_model: gpt-5-nano    # for very short tasks, low cost/latency
  fallback_order:
    - gpt-5
    - gpt-5-mini
    - gpt-4

policies:
  # task labels → model keys
  chat_high_quality: primary_model
  tool_reasoning: primary_model
  moderation_light: classification_model
  intent_detect: routing_model
  title_summarize: routing_model
  rate_limited_burst: ultra_light_model
```

**Rules**
- All call sites request a **task label** (e.g., `chat_high_quality`, `intent_detect`) instead of a raw model string.
- Resolver returns `(model_name, options)` and transparently handles **fallbacks** on retriable errors.
- Fallback triggers: HTTP 429, connect timeouts, 5xx, vendor‑declared incident flag, or policy override.
- Full **audit** trail: selected model, retries, final model, status, latency, tokens used.

### 4.2 Failure & Fallback
- **Retry budget:** exponential backoff with jitter; default 2 retries before stepping down one tier.
- **Circuit breaker:** if error rate for a model > X% over Y minutes, temporarily demote in the chain.
- **Kill‑switch:** single env var to force **all** tasks to `gpt-4` (e.g., `MODEL_FORCE_BACKLEVEL=gpt-4`).

### 4.3 Configuration
Environment variables (12‑factor, used to populate `config/models.yaml` at boot or read directly):
- `OPENAI_API_KEY` (secret)
- `MODEL_DEFAULT_PRIMARY` (default `gpt-5`)
- `MODEL_DEFAULT_CLASSIFICATION` (default `gpt-5-mini`)
- `MODEL_DEFAULT_ULTRALIGHT` (default `gpt-5-nano`)
- `MODEL_FALLBACK_ORDER` (CSV; default `gpt-5,gpt-5-mini,gpt-4`)
- `MODEL_FORCE_BACKLEVEL` (optional; empty = disabled)
- `MODEL_TIMEOUT_SEC` (per‑call deadline; default 30s DEV, 45s TEST/PROD)
- `MODEL_MAX_RETRIES` (default 2)
- `MODEL_LOG_VERBOSE` (true/false)

### 4.4 Mini vs Nano Placement (Initial Policy)
- **GPT‑5 (full):** primary chat/agent steps, tool‑use reasoning, long‑form generation, complex RAG answers.
- **GPT‑5‑mini:** intent detection, light moderation, short summaries/titles, step‑down chat under load.
- **GPT‑5‑nano:** very short classification/enrichment, high‑QPS burst control tasks, ephemeral titles/tags.

> Policy can be tuned via config without code changes.

---

## 5) User Stories & Acceptance Criteria

### US‑1 — As a GM using chat, I get GPT‑5 quality by default
**Given** the chat/agent UI in DEV  
**When** I send a normal question  
**Then** responses are generated by **GPT‑5** (recorded in logs/telemetry)  
**And** if GPT‑5 is unavailable, the system falls back to **GPT‑5‑mini**, then **GPT‑4**  
**And** I am unaware of internals unless an error occurs after exhausting fallbacks.

### US‑2 — As an operator, I can change model policies without redeploy
**Given** the service is running  
**When** I edit `config/models.yaml` or env vars  
**Then** subsequent requests respect new model mappings and fallback order  
**And** the effective config is emitted to logs at boot (sans secrets).

### US‑3 — As a developer, I can smoke‑test the API in DEV
**Given** DEV environment with valid `OPENAI_API_KEY`  
**When** I run `scripts/dev/smoke_llm.ps1` or `python -m tools.smoke_llm`  
**Then** the script calls the **configured primary** model with a known prompt  
**And** verifies non‑empty text, reasonable latency (< 3s target in DEV), and no exceptions  
**And** prints the resolved model and fallback hops (if any).

### US‑4 — As a pipeline engineer, I can route light tasks to mini/nano
**Given** LangFlow and batch workers  
**When** a task is labeled `intent_detect`, `moderation_light`, or `title_summarize`  
**Then** the resolver selects **GPT‑5‑mini** (or **nano** where configured)  
**And** logs include `task_label` and `selected_model` for verification.

### US‑5 — As an SRE, I can force a backlevel during an incident
**Given** an outage for GPT‑5  
**When** I set `MODEL_FORCE_BACKLEVEL=gpt-4` and reload the service  
**Then** all tasks use GPT‑4 immediately, confirmed by logs and smoke tests.

### US‑6 — As finance, I can estimate spend deltas post‑cutover
**Given** logging of tokens, model, and latency  
**When** I export usage metrics for a period  
**Then** I can compare GPT‑5 vs historical GPT‑4 usage to validate cost assumptions.

---

## 6) Test Plan

### 6.1 Unit Tests (Python)
- **ModelResolver policy mapping**  
  - Given task label → returns expected model
  - Env overrides respected
- **Fallback logic**  
  - 429/5xx/timeouts → retries then step‑down; captures final model
  - Circuit breaker demotes after threshold breach
- **Kill‑switch**  
  - `MODEL_FORCE_BACKLEVEL` forces all resolutions

### 6.2 Integration Tests (DEV)
- **Smoke: Chat completion happy path**  
  - Prompt: “Return the word OK.” → non‑empty result from GPT‑5 (or fallback)  
  - Assert latency < 3s (non‑strict), HTTP 200
- **Fallback path** (inject “fail primary” via mock flag)  
  - Primary fails → mini used → success; logs show hop count = 1
- **Nano path**  
  - Label `title_summarize` → nano selected; output ≤ 20 tokens
- **Rate‑limit behavior**  
  - Burst 20 parallel calls → some 429; resolver retries and succeeds without user‑visible error
- **Telemetry**  
  - Logs include: request_id, task_label, model, retries, latency_ms, tokens_in/out

### 6.3 E2E (UI / Discord)
- **UI Chat**: ask short/long queries; confirm model usage via admin log view
- **Discord Bridge**: short “title me this” → mini/nano route; long answer → primary route

### 6.4 Negative / Security
- Invalid API key → clear error surfaced in operator logs, masked in user UI
- Empty prompt guardrails → 400 with helpful message (no API call)
- Timeouts → user gets polite retry suggestion; system continues with fallback policy

---

## 7) Implementation Checklist

1. Add **ModelResolver** module and tests.  
2. Create `config/models.yaml` + env var bindings.  
3. Migrate all call sites to task‑label API (no raw model strings).  
4. Update LangFlow custom nodes to use resolver.  
5. Add **smoke test** scripts: `tools/smoke_llm.py`, `scripts/dev/smoke_llm.ps1`.  
6. Add logging fields in API client and wire to metrics.  
7. Add circuit‑breaker + retry/backoff helper.  
8. Add **kill‑switch** handling and admin doc.  
9. Update README/ops runbook for model policy changes.  
10. Create E2E tests in UI and Discord bridge paths.  

---

## 8) Rollout & Validation

- **DEV**: implement + pass smoke/integration.  
- **TEST**: enable for 50% traffic via config; monitor latency and fallback rate.  
- **PROD**: enable 100% after 24h stable in TEST.  
- **Rollback**: set `MODEL_FORCE_BACKLEVEL=gpt-4` and redeploy config‑only change.

---

## 9) Observability

- Metrics: `llm_calls_total`, `llm_latency_ms`, `llm_retries_total`, `llm_fallback_hops`, `llm_errors_total`  
- Dimensions: `model`, `task_label`, `env`, `route` (ui, discord, batch), `success`  
- Log sample (JSON):  
  ```json
  {
    "ts": "2025-09-16T14:03:22Z",
    "req": "abc123",
    "task": "chat_high_quality",
    "model": "gpt-5",
    "retries": 0,
    "fallback_hops": 0,
    "latency_ms": 842,
    "tok_in": 325,
    "tok_out": 201,
    "env": "DEV"
  }
  ```

---

## 10) Risks & Mitigations

- **Model differences** change output style → capture in prompts if needed; acceptance tests compare key semantics, not exact text.  
- **Vendor incident / rate limits** → fallback chain + kill‑switch.  
- **Cost variance** → record token usage; finance dashboard comparison (US‑6).  
- **Latency spikes** → policy can shift more tasks to mini/nano during peaks.

---

## 11) Acceptance Criteria (DoD)

- All non‑embedding call sites use **ModelResolver**.  
- DEV smoke test passes and prints resolved model names.  
- Integration tests show successful fallback behavior.  
- E2E UI and Discord tests validate correct routing to mini/nano.  
- Ops runbook updated; config can force backlevel without rebuild.  
- Metrics visible on the dashboard with model/fallback attribution.

---

## 12) DEV Smoke Test (example)

PowerShell:
```powershell
$env:OPENAI_API_KEY = "<redacted>"
$env:MODEL_DEFAULT_PRIMARY = "gpt-5"
python -m tools.smoke_llm --prompt "Return the string OK"
```

Python:
```python
# tools/smoke_llm.py
from ttrpg.core.llm_client import llm_call
print(llm_call("Return the string OK", task_label="chat_high_quality"))
```

**Expected:** `OK` (or short equivalent), 200, latency under soft target, logs show `model=gpt-5` or fallback model.
